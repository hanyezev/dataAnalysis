{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:22:40.215946Z",
     "start_time": "2020-12-17T03:22:40.211944Z"
    }
   },
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch import nn, optim\n",
    "from    torchvision import transforms, datasets\n",
    "import  visdom\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:18:57.293730Z",
     "start_time": "2020-12-17T03:18:57.282729Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "\n",
    "        # [b, 784] => [b, 20]\n",
    "        # u: [b, 10]\n",
    "        # sigma: [b, 10]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # [b, 20] => [b, 784]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.criteon = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batchsz = x.size(0)\n",
    "        # flatten\n",
    "        x = x.view(batchsz, 784)\n",
    "        # encoder\n",
    "        # [b, 20], including mean and sigma\n",
    "        h_ = self.encoder(x)\n",
    "        # [b, 20] => [b, 10] and [b, 10]\n",
    "        mu, sigma = h_.chunk(2, dim=1)\n",
    "        # reparametrize trick, epison~N(0, 1)\n",
    "        h = mu + sigma * torch.randn_like(sigma)\n",
    "\n",
    "        # decoder\n",
    "        x_hat = self.decoder(h)\n",
    "        # reshape\n",
    "        x_hat = x_hat.view(batchsz, 1, 28, 28)\n",
    "\n",
    "        kld = 0.5 * torch.sum(\n",
    "            torch.pow(mu, 2) +\n",
    "            torch.pow(sigma, 2) -\n",
    "            torch.log(1e-8 + torch.pow(sigma, 2)) - 1\n",
    "        ) / (batchsz*28*28)\n",
    "\n",
    "        return x_hat, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:18:57.506666Z",
     "start_time": "2020-12-17T03:18:57.463661Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST('mnist', True, transform=transforms.Compose([transforms.ToTensor()]), download=False)\n",
    "mnist_test = datasets.MNIST('mnist', False, transform=transforms.Compose([transforms.ToTensor()]), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:18:58.142686Z",
     "start_time": "2020-12-17T03:18:58.138684Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "mnist_test = DataLoader(mnist_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:19:01.146676Z",
     "start_time": "2020-12-17T03:19:01.128656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, _ = iter(mnist_train).next()\n",
    "print('x:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = visdom.Visdom(use_incoming_socket=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T03:13:59.456099Z",
     "start_time": "2020-12-16T03:13:59.449099Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T03:14:00.239095Z",
     "start_time": "2020-12-16T03:14:00.235094Z"
    }
   },
   "outputs": [],
   "source": [
    "criteon = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T03:14:01.036363Z",
     "start_time": "2020-12-16T03:14:01.023367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=20, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=784, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       "  (criteon): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:47:23.699678Z",
     "start_time": "2020-12-16T03:17:57.979334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.07325191050767899 kld: 0.017683329060673714\n",
      "1 loss: 0.048956796526908875 kld: 0.006203962955623865\n",
      "2 loss: 0.04813229292631149 kld: 0.007824454456567764\n",
      "3 loss: 0.04703172296285629 kld: 0.007814062759280205\n",
      "4 loss: 0.0467158742249012 kld: 0.00792806874960661\n",
      "5 loss: 0.05149685591459274 kld: 0.008520587347447872\n",
      "6 loss: 0.050130680203437805 kld: 0.008276141248643398\n",
      "7 loss: 0.044426411390304565 kld: 0.008716377429664135\n",
      "8 loss: 0.05141933634877205 kld: 0.008179866708815098\n",
      "9 loss: 0.0463092215359211 kld: 0.00917123258113861\n",
      "10 loss: 0.051698166877031326 kld: 0.009055194444954395\n",
      "11 loss: 0.044617678970098495 kld: 0.008537468500435352\n",
      "12 loss: 0.04720044881105423 kld: 0.0082466471940279\n",
      "13 loss: 0.051399149000644684 kld: 0.009027311578392982\n",
      "14 loss: 0.03806211054325104 kld: 0.0077248793095350266\n",
      "15 loss: 0.045137397944927216 kld: 0.009074234403669834\n",
      "16 loss: 0.04852191358804703 kld: 0.008916136808693409\n",
      "17 loss: 0.048965368419885635 kld: 0.009279765188694\n",
      "18 loss: 0.05160927399992943 kld: 0.009300420992076397\n",
      "19 loss: 0.045397933572530746 kld: 0.009130269289016724\n",
      "20 loss: 0.05126430466771126 kld: 0.008789226412773132\n",
      "21 loss: 0.04756096005439758 kld: 0.008895724080502987\n",
      "22 loss: 0.04413335770368576 kld: 0.00886751152575016\n",
      "23 loss: 0.046279601752758026 kld: 0.00962085835635662\n",
      "24 loss: 0.0456339567899704 kld: 0.008597132749855518\n",
      "25 loss: 0.04717152938246727 kld: 0.009583684615790844\n",
      "26 loss: 0.04658730700612068 kld: 0.009371805936098099\n",
      "27 loss: 0.044261761009693146 kld: 0.009598635137081146\n",
      "28 loss: 0.04213337227702141 kld: 0.009279012680053711\n",
      "29 loss: 0.0455622673034668 kld: 0.009352706372737885\n",
      "30 loss: 0.046809300780296326 kld: 0.009354175068438053\n",
      "31 loss: 0.04777214303612709 kld: 0.009238488972187042\n",
      "32 loss: 0.042655251920223236 kld: 0.009489028714597225\n",
      "33 loss: 0.04892082139849663 kld: 0.00928043108433485\n",
      "34 loss: 0.04418177530169487 kld: 0.00884498655796051\n",
      "35 loss: 0.04950578883290291 kld: 0.009078721515834332\n",
      "36 loss: 0.04552481696009636 kld: 0.009505581110715866\n",
      "37 loss: 0.04426117241382599 kld: 0.008956781588494778\n",
      "38 loss: 0.03987421467900276 kld: 0.009054447524249554\n",
      "39 loss: 0.04632370173931122 kld: 0.009550144895911217\n",
      "40 loss: 0.04743799567222595 kld: 0.009655513800680637\n",
      "41 loss: 0.045452430844306946 kld: 0.009172054007649422\n",
      "42 loss: 0.04216984659433365 kld: 0.009134511463344097\n",
      "43 loss: 0.04529445618391037 kld: 0.009438993409276009\n",
      "44 loss: 0.038184668868780136 kld: 0.008608040399849415\n",
      "45 loss: 0.04759887233376503 kld: 0.010043065994977951\n",
      "46 loss: 0.038669560104608536 kld: 0.009393212385475636\n",
      "47 loss: 0.04882849007844925 kld: 0.01004745252430439\n",
      "48 loss: 0.042042769491672516 kld: 0.009330925531685352\n",
      "49 loss: 0.046927861869335175 kld: 0.009717566892504692\n",
      "50 loss: 0.042640428990125656 kld: 0.008936339057981968\n",
      "51 loss: 0.04652892053127289 kld: 0.009401042014360428\n",
      "52 loss: 0.04486287757754326 kld: 0.009538672864437103\n",
      "53 loss: 0.04236546903848648 kld: 0.009461888112127781\n",
      "54 loss: 0.04265511780977249 kld: 0.009360579773783684\n",
      "55 loss: 0.04756663739681244 kld: 0.009412622079253197\n",
      "56 loss: 0.04136677086353302 kld: 0.009476801380515099\n",
      "57 loss: 0.04925420135259628 kld: 0.009439443238079548\n",
      "58 loss: 0.047080714255571365 kld: 0.008871514350175858\n",
      "59 loss: 0.04164452850818634 kld: 0.009561995975673199\n",
      "60 loss: 0.04702373594045639 kld: 0.009501347318291664\n",
      "61 loss: 0.041415125131607056 kld: 0.009614341892302036\n",
      "62 loss: 0.042233601212501526 kld: 0.009848445653915405\n",
      "63 loss: 0.04462067410349846 kld: 0.009099792689085007\n",
      "64 loss: 0.04933745041489601 kld: 0.009477966465055943\n",
      "65 loss: 0.044796571135520935 kld: 0.008558169938623905\n",
      "66 loss: 0.045351095497608185 kld: 0.009223937056958675\n",
      "67 loss: 0.0506897009909153 kld: 0.009418695233762264\n",
      "68 loss: 0.046225689351558685 kld: 0.009385261684656143\n",
      "69 loss: 0.0429522879421711 kld: 0.009252220392227173\n",
      "70 loss: 0.04308438301086426 kld: 0.009462348185479641\n",
      "71 loss: 0.04830034077167511 kld: 0.009126215241849422\n",
      "72 loss: 0.047532323747873306 kld: 0.009167499840259552\n",
      "73 loss: 0.04513115808367729 kld: 0.00956446211785078\n",
      "74 loss: 0.04768521338701248 kld: 0.00996506866067648\n",
      "75 loss: 0.04830799251794815 kld: 0.009354852139949799\n",
      "76 loss: 0.04674601927399635 kld: 0.008943691849708557\n",
      "77 loss: 0.047448962926864624 kld: 0.009727844037115574\n",
      "78 loss: 0.044610645622015 kld: 0.009900365956127644\n",
      "79 loss: 0.042410992085933685 kld: 0.009172545745968819\n",
      "80 loss: 0.04823211953043938 kld: 0.009208443574607372\n",
      "81 loss: 0.049765389412641525 kld: 0.009205501526594162\n",
      "82 loss: 0.03842097893357277 kld: 0.009480150416493416\n",
      "83 loss: 0.04447098448872566 kld: 0.00948752835392952\n",
      "84 loss: 0.043332021683454514 kld: 0.00955402571707964\n",
      "85 loss: 0.047665949910879135 kld: 0.009358075447380543\n",
      "86 loss: 0.04029543697834015 kld: 0.0092541528865695\n",
      "87 loss: 0.04848983511328697 kld: 0.010445388965308666\n",
      "88 loss: 0.043328769505023956 kld: 0.009334485977888107\n",
      "89 loss: 0.0510394424200058 kld: 0.010264754295349121\n",
      "90 loss: 0.04374246671795845 kld: 0.010427980683743954\n",
      "91 loss: 0.04281870275735855 kld: 0.00880266260355711\n",
      "92 loss: 0.04613735154271126 kld: 0.009190480224788189\n",
      "93 loss: 0.042846132069826126 kld: 0.00954766571521759\n",
      "94 loss: 0.03976266086101532 kld: 0.009078308939933777\n",
      "95 loss: 0.048260219395160675 kld: 0.00969766452908516\n",
      "96 loss: 0.0415935181081295 kld: 0.009589292109012604\n",
      "97 loss: 0.0472656674683094 kld: 0.010096442885696888\n",
      "98 loss: 0.04644398391246796 kld: 0.009057768620550632\n",
      "99 loss: 0.0454479418694973 kld: 0.009543799795210361\n",
      "100 loss: 0.04475540667772293 kld: 0.00895078293979168\n",
      "101 loss: 0.04634683206677437 kld: 0.009292486123740673\n",
      "102 loss: 0.04584098607301712 kld: 0.009064613841474056\n",
      "103 loss: 0.04906614497303963 kld: 0.0106948958709836\n",
      "104 loss: 0.045502807945013046 kld: 0.010638085193932056\n",
      "105 loss: 0.042613863945007324 kld: 0.009737035259604454\n",
      "106 loss: 0.046117838472127914 kld: 0.009235687553882599\n",
      "107 loss: 0.039439648389816284 kld: 0.009088924154639244\n",
      "108 loss: 0.047206006944179535 kld: 0.009967707097530365\n",
      "109 loss: 0.04223177582025528 kld: 0.008877537213265896\n",
      "110 loss: 0.04353701323270798 kld: 0.00946048367768526\n",
      "111 loss: 0.052260227501392365 kld: 0.00948324054479599\n",
      "112 loss: 0.046793170273303986 kld: 0.009480292908847332\n",
      "113 loss: 0.04465483874082565 kld: 0.009378325194120407\n",
      "114 loss: 0.043433185666799545 kld: 0.009635032154619694\n",
      "115 loss: 0.042277030646800995 kld: 0.00917736068367958\n",
      "116 loss: 0.05031575635075569 kld: 0.009505979716777802\n",
      "117 loss: 0.047258034348487854 kld: 0.009683793410658836\n",
      "118 loss: 0.046871066093444824 kld: 0.010145372711122036\n",
      "119 loss: 0.0423983596265316 kld: 0.009603932499885559\n",
      "120 loss: 0.047996412962675095 kld: 0.009569890797138214\n",
      "121 loss: 0.043388236314058304 kld: 0.009805778972804546\n",
      "122 loss: 0.0482662059366703 kld: 0.009177111089229584\n",
      "123 loss: 0.04774576798081398 kld: 0.009964857250452042\n",
      "124 loss: 0.04734760522842407 kld: 0.009689089842140675\n",
      "125 loss: 0.043323367834091187 kld: 0.009407566860318184\n",
      "126 loss: 0.04042652249336243 kld: 0.0095240268856287\n",
      "127 loss: 0.04021075367927551 kld: 0.009081361815333366\n",
      "128 loss: 0.04941426217556 kld: 0.009862213395535946\n",
      "129 loss: 0.04736119881272316 kld: 0.010887411423027515\n",
      "130 loss: 0.048229265958070755 kld: 0.00943050254136324\n",
      "131 loss: 0.04402320832014084 kld: 0.009419865906238556\n",
      "132 loss: 0.04162917286157608 kld: 0.008864309638738632\n",
      "133 loss: 0.04324676841497421 kld: 0.00988711230456829\n",
      "134 loss: 0.04435247927904129 kld: 0.009181156754493713\n",
      "135 loss: 0.0452420637011528 kld: 0.009645676240324974\n",
      "136 loss: 0.044329043477773666 kld: 0.009470862336456776\n",
      "137 loss: 0.04403405636548996 kld: 0.010157587938010693\n",
      "138 loss: 0.04196830838918686 kld: 0.009599672630429268\n",
      "139 loss: 0.044116366654634476 kld: 0.009441730566322803\n",
      "140 loss: 0.04646420478820801 kld: 0.010214165784418583\n",
      "141 loss: 0.0426495224237442 kld: 0.009943144395947456\n",
      "142 loss: 0.04370769113302231 kld: 0.009172243997454643\n",
      "143 loss: 0.04368189722299576 kld: 0.009833624586462975\n",
      "144 loss: 0.04930170625448227 kld: 0.008787776343524456\n",
      "145 loss: 0.04343414306640625 kld: 0.008814133703708649\n",
      "146 loss: 0.04191048815846443 kld: 0.009903065860271454\n",
      "147 loss: 0.04476204887032509 kld: 0.010269895195960999\n",
      "148 loss: 0.04153290390968323 kld: 0.009181861765682697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 loss: 0.046198952943086624 kld: 0.009330139495432377\n",
      "150 loss: 0.04643646627664566 kld: 0.009423859417438507\n",
      "151 loss: 0.048245400190353394 kld: 0.009266117587685585\n",
      "152 loss: 0.04093409329652786 kld: 0.008479460142552853\n",
      "153 loss: 0.040981318801641464 kld: 0.009870738722383976\n",
      "154 loss: 0.04640789330005646 kld: 0.009199514985084534\n",
      "155 loss: 0.043319664895534515 kld: 0.009655829519033432\n",
      "156 loss: 0.04760870710015297 kld: 0.009455283172428608\n",
      "157 loss: 0.04425114020705223 kld: 0.00992922205477953\n",
      "158 loss: 0.046265602111816406 kld: 0.009494359605014324\n",
      "159 loss: 0.047667764127254486 kld: 0.00899045355618\n",
      "160 loss: 0.043427638709545135 kld: 0.009602256119251251\n",
      "161 loss: 0.047472499310970306 kld: 0.009851501323282719\n",
      "162 loss: 0.04680495336651802 kld: 0.008952760137617588\n",
      "163 loss: 0.043476518243551254 kld: 0.00965746957808733\n",
      "164 loss: 0.042239606380462646 kld: 0.009317144751548767\n",
      "165 loss: 0.041390568017959595 kld: 0.010507535189390182\n",
      "166 loss: 0.041437502950429916 kld: 0.009920206852257252\n",
      "167 loss: 0.04136355221271515 kld: 0.010166250169277191\n",
      "168 loss: 0.04566488415002823 kld: 0.009543894790112972\n",
      "169 loss: 0.04555139318108559 kld: 0.009423048235476017\n",
      "170 loss: 0.04368775337934494 kld: 0.00939103588461876\n",
      "171 loss: 0.04814784973859787 kld: 0.009456920437514782\n",
      "172 loss: 0.04797310009598732 kld: 0.010558422654867172\n",
      "173 loss: 0.04774881526827812 kld: 0.009945631958544254\n",
      "174 loss: 0.04530933499336243 kld: 0.00988123007118702\n",
      "175 loss: 0.04390036687254906 kld: 0.010128105990588665\n",
      "176 loss: 0.042638763785362244 kld: 0.008714700117707253\n",
      "177 loss: 0.040162671357393265 kld: 0.009896013885736465\n",
      "178 loss: 0.041275620460510254 kld: 0.009264345280826092\n",
      "179 loss: 0.04255899041891098 kld: 0.0096256323158741\n",
      "180 loss: 0.04260735213756561 kld: 0.00950434897094965\n",
      "181 loss: 0.041555050760507584 kld: 0.009332186542451382\n",
      "182 loss: 0.04432502016425133 kld: 0.009433973580598831\n",
      "183 loss: 0.0483558289706707 kld: 0.009957574307918549\n",
      "184 loss: 0.04258280247449875 kld: 0.009390261955559254\n",
      "185 loss: 0.04470614716410637 kld: 0.00962971244007349\n",
      "186 loss: 0.04005725309252739 kld: 0.008865166455507278\n",
      "187 loss: 0.04625774547457695 kld: 0.009591102600097656\n",
      "188 loss: 0.050564318895339966 kld: 0.009770152159035206\n",
      "189 loss: 0.04123462736606598 kld: 0.009953932836651802\n",
      "190 loss: 0.04353083670139313 kld: 0.009077339433133602\n",
      "191 loss: 0.03918701782822609 kld: 0.009854436852037907\n",
      "192 loss: 0.0401778481900692 kld: 0.009190416894853115\n",
      "193 loss: 0.049497850239276886 kld: 0.009930917993187904\n",
      "194 loss: 0.04379279911518097 kld: 0.00943787768483162\n",
      "195 loss: 0.04341302812099457 kld: 0.00957784615457058\n",
      "196 loss: 0.051842741668224335 kld: 0.009678857401013374\n",
      "197 loss: 0.04592618718743324 kld: 0.009677965193986893\n",
      "198 loss: 0.039540085941553116 kld: 0.009510147385299206\n",
      "199 loss: 0.04034169018268585 kld: 0.010730096139013767\n",
      "200 loss: 0.05091412365436554 kld: 0.010048700496554375\n",
      "201 loss: 0.040706098079681396 kld: 0.009234623983502388\n",
      "202 loss: 0.04316151142120361 kld: 0.009631861001253128\n",
      "203 loss: 0.03707605227828026 kld: 0.010024192743003368\n",
      "204 loss: 0.04656951129436493 kld: 0.0099391033872962\n",
      "205 loss: 0.043609630316495895 kld: 0.009427927434444427\n",
      "206 loss: 0.0431145541369915 kld: 0.009201046079397202\n",
      "207 loss: 0.040750838816165924 kld: 0.00918046198785305\n",
      "208 loss: 0.04156913608312607 kld: 0.008838819339871407\n",
      "209 loss: 0.04104948788881302 kld: 0.009455278515815735\n",
      "210 loss: 0.04531252011656761 kld: 0.010160268284380436\n",
      "211 loss: 0.04417039826512337 kld: 0.008936350233852863\n",
      "212 loss: 0.04480515420436859 kld: 0.009559771046042442\n",
      "213 loss: 0.04677766188979149 kld: 0.009724748320877552\n",
      "214 loss: 0.046684324741363525 kld: 0.010646152310073376\n",
      "215 loss: 0.04069514200091362 kld: 0.009283140301704407\n",
      "216 loss: 0.048577044159173965 kld: 0.009517828933894634\n",
      "217 loss: 0.03922547399997711 kld: 0.009628672152757645\n",
      "218 loss: 0.04534551501274109 kld: 0.010141683742403984\n",
      "219 loss: 0.04341394454240799 kld: 0.009108437225222588\n",
      "220 loss: 0.045557182282209396 kld: 0.00908605009317398\n",
      "221 loss: 0.05031805485486984 kld: 0.00948011688888073\n",
      "222 loss: 0.04131138697266579 kld: 0.009784803725779057\n",
      "223 loss: 0.04749853536486626 kld: 0.009671851992607117\n",
      "224 loss: 0.042795389890670776 kld: 0.009983135387301445\n",
      "225 loss: 0.042366061359643936 kld: 0.009373619221150875\n",
      "226 loss: 0.04264243692159653 kld: 0.010277335532009602\n",
      "227 loss: 0.04094114154577255 kld: 0.008790917694568634\n",
      "228 loss: 0.044156674295663834 kld: 0.009218708612024784\n",
      "229 loss: 0.038221120834350586 kld: 0.009493574500083923\n",
      "230 loss: 0.04051883518695831 kld: 0.008790591731667519\n",
      "231 loss: 0.040989264845848083 kld: 0.010032765567302704\n",
      "232 loss: 0.046039924025535583 kld: 0.01008596271276474\n",
      "233 loss: 0.04649742692708969 kld: 0.010006706230342388\n",
      "234 loss: 0.04560117423534393 kld: 0.00964544527232647\n",
      "235 loss: 0.04467516764998436 kld: 0.009808179922401905\n",
      "236 loss: 0.049977488815784454 kld: 0.009692618623375893\n",
      "237 loss: 0.04401553422212601 kld: 0.009454680606722832\n",
      "238 loss: 0.044653844088315964 kld: 0.010357506573200226\n",
      "239 loss: 0.04519479349255562 kld: 0.009800761938095093\n",
      "240 loss: 0.04463803395628929 kld: 0.009950797073543072\n",
      "241 loss: 0.04354694485664368 kld: 0.00869784690439701\n",
      "242 loss: 0.04751560837030411 kld: 0.010154311545193195\n",
      "243 loss: 0.04034871980547905 kld: 0.009109743870794773\n",
      "244 loss: 0.04235530644655228 kld: 0.009452307596802711\n",
      "245 loss: 0.05046205222606659 kld: 0.010115636512637138\n",
      "246 loss: 0.048892565071582794 kld: 0.00998497661203146\n",
      "247 loss: 0.04188435152173042 kld: 0.008847102522850037\n",
      "248 loss: 0.04830267280340195 kld: 0.009425436146557331\n",
      "249 loss: 0.041417285799980164 kld: 0.009529689326882362\n",
      "250 loss: 0.04198695719242096 kld: 0.009334923699498177\n",
      "251 loss: 0.046781621873378754 kld: 0.009624103084206581\n",
      "252 loss: 0.04218342527747154 kld: 0.009673681110143661\n",
      "253 loss: 0.04471103847026825 kld: 0.009909948334097862\n",
      "254 loss: 0.04580458253622055 kld: 0.00991226825863123\n",
      "255 loss: 0.04384268820285797 kld: 0.00958300568163395\n",
      "256 loss: 0.04358600452542305 kld: 0.009900320321321487\n",
      "257 loss: 0.0412338450551033 kld: 0.009625005535781384\n",
      "258 loss: 0.04179001599550247 kld: 0.009411316365003586\n",
      "259 loss: 0.044427357614040375 kld: 0.00959786307066679\n",
      "260 loss: 0.043485127389431 kld: 0.009356620721518993\n",
      "261 loss: 0.049803562462329865 kld: 0.00988644827157259\n",
      "262 loss: 0.041424281895160675 kld: 0.009445944800972939\n",
      "263 loss: 0.045613404363393784 kld: 0.0097556933760643\n",
      "264 loss: 0.047678135335445404 kld: 0.009800556115806103\n",
      "265 loss: 0.04246578365564346 kld: 0.009825051762163639\n",
      "266 loss: 0.04446762800216675 kld: 0.009215975180268288\n",
      "267 loss: 0.04732169210910797 kld: 0.010277719236910343\n",
      "268 loss: 0.04523129761219025 kld: 0.009661635383963585\n",
      "269 loss: 0.048620931804180145 kld: 0.010364475660026073\n",
      "270 loss: 0.042905502021312714 kld: 0.009796371683478355\n",
      "271 loss: 0.049312248826026917 kld: 0.010434111580252647\n",
      "272 loss: 0.04518663138151169 kld: 0.0093118567019701\n",
      "273 loss: 0.04723565652966499 kld: 0.010067392140626907\n",
      "274 loss: 0.04320480301976204 kld: 0.009900342673063278\n",
      "275 loss: 0.04498114436864853 kld: 0.009932716377079487\n",
      "276 loss: 0.04488639906048775 kld: 0.010697734542191029\n",
      "277 loss: 0.04542921483516693 kld: 0.009425693191587925\n",
      "278 loss: 0.038993820548057556 kld: 0.009472116827964783\n",
      "279 loss: 0.03990636765956879 kld: 0.009178949519991875\n",
      "280 loss: 0.048937149345874786 kld: 0.010011602193117142\n",
      "281 loss: 0.0402572862803936 kld: 0.009593090042471886\n",
      "282 loss: 0.05038750171661377 kld: 0.010833187960088253\n",
      "283 loss: 0.0447087325155735 kld: 0.009639128111302853\n",
      "284 loss: 0.04681944102048874 kld: 0.010101727209985256\n",
      "285 loss: 0.04444079101085663 kld: 0.009494895115494728\n",
      "286 loss: 0.04451577737927437 kld: 0.010239061899483204\n",
      "287 loss: 0.04221758246421814 kld: 0.009541952982544899\n",
      "288 loss: 0.04714386165142059 kld: 0.010481726378202438\n",
      "289 loss: 0.04078201204538345 kld: 0.009381982497870922\n",
      "290 loss: 0.05123509466648102 kld: 0.009787365794181824\n",
      "291 loss: 0.0445115752518177 kld: 0.009610939770936966\n",
      "292 loss: 0.051831185817718506 kld: 0.01028517447412014\n",
      "293 loss: 0.04835386574268341 kld: 0.009382796473801136\n",
      "294 loss: 0.037992000579833984 kld: 0.00935855507850647\n",
      "295 loss: 0.04704225808382034 kld: 0.01086029689759016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 loss: 0.04528867080807686 kld: 0.010691010393202305\n",
      "297 loss: 0.044421978294849396 kld: 0.009740461595356464\n",
      "298 loss: 0.04662606865167618 kld: 0.01030943263322115\n",
      "299 loss: 0.03952508047223091 kld: 0.009123452007770538\n",
      "300 loss: 0.05017346888780594 kld: 0.010204884223639965\n",
      "301 loss: 0.04720431566238403 kld: 0.010538021102547646\n",
      "302 loss: 0.046429142355918884 kld: 0.0099199702963233\n",
      "303 loss: 0.04683499038219452 kld: 0.009679744951426983\n",
      "304 loss: 0.049577079713344574 kld: 0.009940994903445244\n",
      "305 loss: 0.04196963831782341 kld: 0.009566581808030605\n",
      "306 loss: 0.04659895971417427 kld: 0.009853796102106571\n",
      "307 loss: 0.04491090029478073 kld: 0.009692935273051262\n",
      "308 loss: 0.043321870267391205 kld: 0.00941340159624815\n",
      "309 loss: 0.044671431183815 kld: 0.01030008401721716\n",
      "310 loss: 0.045286983251571655 kld: 0.009551960974931717\n",
      "311 loss: 0.0447261743247509 kld: 0.010248369537293911\n",
      "312 loss: 0.043523158878088 kld: 0.010767542757093906\n",
      "313 loss: 0.04210415482521057 kld: 0.009174482896924019\n",
      "314 loss: 0.049110181629657745 kld: 0.010009790770709515\n",
      "315 loss: 0.0382201112806797 kld: 0.009852881543338299\n",
      "316 loss: 0.04037942364811897 kld: 0.010246449150145054\n",
      "317 loss: 0.04062838479876518 kld: 0.008840893395245075\n",
      "318 loss: 0.04277852922677994 kld: 0.01009316835552454\n",
      "319 loss: 0.04351959377527237 kld: 0.00964775588363409\n",
      "320 loss: 0.04581792652606964 kld: 0.009744420647621155\n",
      "321 loss: 0.0453781895339489 kld: 0.009400431998074055\n",
      "322 loss: 0.043919313699007034 kld: 0.009119100868701935\n",
      "323 loss: 0.04316418990492821 kld: 0.009714431129395962\n",
      "324 loss: 0.03922513127326965 kld: 0.009824946522712708\n",
      "325 loss: 0.041914280503988266 kld: 0.009660645388066769\n",
      "326 loss: 0.04773710295557976 kld: 0.009651735424995422\n",
      "327 loss: 0.044283729046583176 kld: 0.009584254585206509\n",
      "328 loss: 0.03939659893512726 kld: 0.010699916630983353\n",
      "329 loss: 0.045337725430727005 kld: 0.009414675645530224\n",
      "330 loss: 0.04082469642162323 kld: 0.009581291116774082\n",
      "331 loss: 0.04780171066522598 kld: 0.009704350493848324\n",
      "332 loss: 0.04531997814774513 kld: 0.009723369963467121\n",
      "333 loss: 0.04767811670899391 kld: 0.009894671849906445\n",
      "334 loss: 0.04867421090602875 kld: 0.008944464847445488\n",
      "335 loss: 0.04271232709288597 kld: 0.009359733201563358\n",
      "336 loss: 0.039789024740457535 kld: 0.009965473785996437\n",
      "337 loss: 0.040476806461811066 kld: 0.009522450156509876\n",
      "338 loss: 0.04277299344539642 kld: 0.010759854689240456\n",
      "339 loss: 0.04624450206756592 kld: 0.009587205946445465\n",
      "340 loss: 0.04449918493628502 kld: 0.0093174884095788\n",
      "341 loss: 0.047229569405317307 kld: 0.010517415590584278\n",
      "342 loss: 0.04189324006438255 kld: 0.009379256516695023\n",
      "343 loss: 0.0465240404009819 kld: 0.009481178596615791\n",
      "344 loss: 0.04110272601246834 kld: 0.009230934083461761\n",
      "345 loss: 0.0478033572435379 kld: 0.0096055306494236\n",
      "346 loss: 0.0396934449672699 kld: 0.009587401524186134\n",
      "347 loss: 0.04284133389592171 kld: 0.009487529285252094\n",
      "348 loss: 0.05155050382018089 kld: 0.008726219646632671\n",
      "349 loss: 0.05103789269924164 kld: 0.010471428744494915\n",
      "350 loss: 0.04905140399932861 kld: 0.010057531297206879\n",
      "351 loss: 0.04120379313826561 kld: 0.009080663323402405\n",
      "352 loss: 0.04294433444738388 kld: 0.009741981513798237\n",
      "353 loss: 0.04601554945111275 kld: 0.009260212071239948\n",
      "354 loss: 0.043510518968105316 kld: 0.010441199876368046\n",
      "355 loss: 0.04481903463602066 kld: 0.00973445177078247\n",
      "356 loss: 0.04474015533924103 kld: 0.010103512555360794\n",
      "357 loss: 0.04147185757756233 kld: 0.009422920644283295\n",
      "358 loss: 0.040215082466602325 kld: 0.009727049618959427\n",
      "359 loss: 0.03998042270541191 kld: 0.009345208294689655\n",
      "360 loss: 0.038373205810785294 kld: 0.009964949451386929\n",
      "361 loss: 0.04365025833249092 kld: 0.009702789597213268\n",
      "362 loss: 0.043290168046951294 kld: 0.009771266020834446\n",
      "363 loss: 0.051462844014167786 kld: 0.009670677594840527\n",
      "364 loss: 0.041870392858982086 kld: 0.009583583101630211\n",
      "365 loss: 0.03903355449438095 kld: 0.008601529523730278\n",
      "366 loss: 0.049839481711387634 kld: 0.009735704399645329\n",
      "367 loss: 0.04531311243772507 kld: 0.00939752534031868\n",
      "368 loss: 0.04663686081767082 kld: 0.009797372855246067\n",
      "369 loss: 0.049326226115226746 kld: 0.00981798954308033\n",
      "370 loss: 0.038499314337968826 kld: 0.009871606715023518\n",
      "371 loss: 0.04585561901330948 kld: 0.009688319638371468\n",
      "372 loss: 0.04293639212846756 kld: 0.009172790683805943\n",
      "373 loss: 0.045012153685092926 kld: 0.00977992732077837\n",
      "374 loss: 0.05074973404407501 kld: 0.009030272252857685\n",
      "375 loss: 0.04533378407359123 kld: 0.010049398057162762\n",
      "376 loss: 0.04688137024641037 kld: 0.009201936423778534\n",
      "377 loss: 0.044334810227155685 kld: 0.009168975986540318\n",
      "378 loss: 0.04936585575342178 kld: 0.010977311059832573\n",
      "379 loss: 0.04369662329554558 kld: 0.009401376359164715\n",
      "380 loss: 0.05183092877268791 kld: 0.009778273291885853\n",
      "381 loss: 0.04513421654701233 kld: 0.009651171043515205\n",
      "382 loss: 0.04410616308450699 kld: 0.010009447112679482\n",
      "383 loss: 0.043448369950056076 kld: 0.009870174340903759\n",
      "384 loss: 0.0411822535097599 kld: 0.010296110063791275\n",
      "385 loss: 0.04217860847711563 kld: 0.010714991018176079\n",
      "386 loss: 0.04262324050068855 kld: 0.009504384361207485\n",
      "387 loss: 0.04543878138065338 kld: 0.00961301289498806\n",
      "388 loss: 0.04450276866555214 kld: 0.008869788609445095\n",
      "389 loss: 0.03988425061106682 kld: 0.0089516406878829\n",
      "390 loss: 0.0467316173017025 kld: 0.010181025601923466\n",
      "391 loss: 0.04605064541101456 kld: 0.00934060849249363\n",
      "392 loss: 0.04620997607707977 kld: 0.009332351386547089\n",
      "393 loss: 0.04527676850557327 kld: 0.010355262085795403\n",
      "394 loss: 0.044753070920705795 kld: 0.009539958089590073\n",
      "395 loss: 0.047749925404787064 kld: 0.009798150509595871\n",
      "396 loss: 0.0417768619954586 kld: 0.009833368472754955\n",
      "397 loss: 0.04251570999622345 kld: 0.009576603770256042\n",
      "398 loss: 0.04534291476011276 kld: 0.009939170442521572\n",
      "399 loss: 0.05046973377466202 kld: 0.009801344946026802\n",
      "400 loss: 0.0463627353310585 kld: 0.009215719997882843\n",
      "401 loss: 0.04766975715756416 kld: 0.009872875176370144\n",
      "402 loss: 0.04350660741329193 kld: 0.009373800829052925\n",
      "403 loss: 0.03891758993268013 kld: 0.009064923971891403\n",
      "404 loss: 0.046319521963596344 kld: 0.009793585166335106\n",
      "405 loss: 0.045356690883636475 kld: 0.010025162249803543\n",
      "406 loss: 0.044157449156045914 kld: 0.009409267455339432\n",
      "407 loss: 0.04627906531095505 kld: 0.009882844053208828\n",
      "408 loss: 0.03774766996502876 kld: 0.008675733581185341\n",
      "409 loss: 0.04056388884782791 kld: 0.00943660270422697\n",
      "410 loss: 0.045498814433813095 kld: 0.009400561451911926\n",
      "411 loss: 0.04485802352428436 kld: 0.010794589295983315\n",
      "412 loss: 0.04207560047507286 kld: 0.009632408618927002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9274d52460ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kld:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwore\\python38\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for batchidx, (x, _) in enumerate(mnist_train):\n",
    "        # [b, 1, 28, 28]\n",
    "        x = x\n",
    "        x_hat, kld = model(x)\n",
    "        loss = criteon(x_hat, x)\n",
    "\n",
    "        if kld is not None:\n",
    "            elbo = - loss - 1.0 * kld\n",
    "            loss = - elbo\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(epoch, 'loss:', loss.item(), 'kld:', kld.item())\n",
    "\n",
    "    x, _ = iter(mnist_test).next()\n",
    "    x = x\n",
    "    with torch.no_grad():\n",
    "        x_hat, kld = model(x)\n",
    "    viz.images(x, nrow=8, win='x', opts=dict(title='x'))\n",
    "    viz.images(x_hat, nrow=8, win='x_hat', opts=dict(title='x_hat'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "env_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
