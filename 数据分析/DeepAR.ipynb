{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAR(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    DeepAR 模型\n",
    "    \"\"\"\n",
    "    def __init__(self, lstm_units):\n",
    "        super().__init__()\n",
    "        # 注意，文章中使用了多层的 LSTM 网络，为了简单起见，本 demo 只使用一层\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.dense_mu = tf.keras.layers.Dense(1)\n",
    "        self.dense_sigma = tf.keras.layers.Dense(1, activation='softplus')\n",
    "\n",
    "    def call(self, inputs, initial_state=None):\n",
    "        outputs, state_h, state_c = self.lstm(inputs, initial_state=initial_state)\n",
    "\n",
    "        mu = self.dense_mu(outputs)\n",
    "        sigma = self.dense_sigma(outputs)\n",
    "        state = [state_h, state_c]\n",
    "\n",
    "        return [mu, sigma, state]\n",
    "\n",
    "def log_gaussian_loss(mu, sigma, y_true):\n",
    "    \"\"\"\n",
    "    Gaussian 损失函数\n",
    "    \"\"\"\n",
    "    return -tf.reduce_sum(tfp.distributions.Normal(loc=mu, scale=sigma).log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "# 实例化模型\n",
    "model = DeepAR(LSTM_UNITS)\n",
    "\n",
    "# 指定优化器\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 使用 RMSE 衡量误差\n",
    "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "# 定义训练步\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mu, sigma, _ = model(x)\n",
    "        loss = log_gaussian_loss(mu, sigma, y)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    rmse(y, mu)\n",
    "\n",
    "# 数据处理（略）\n",
    "# train_data = do_something()\n",
    "\n",
    "# 训练\n",
    "for epoch in range(EPOCHS):\n",
    "    for x, y in train_data:\n",
    "        train_step(x, y)\n",
    "    print('Epoch %d, RMSE %.4f' % (epoch + 1, rmse.result()))\n",
    "    rmse.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "env_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
