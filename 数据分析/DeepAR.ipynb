{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python 3.6\n",
    "#-*-coding:utf-8-*-\n",
    "\n",
    "'''\n",
    "DeepAR Model (Pytorch Implementation)\n",
    "Paper Link: https://arxiv.org/abs/1704.04110\n",
    "Author: Jing Wang (jingw2@foxmail.com)\n",
    "'''\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import util\n",
    "from datetime import date\n",
    "import argparse\n",
    "from progressbar import *\n",
    "\n",
    "class Gaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        '''\n",
    "        Gaussian Likelihood Supports Continuous Data\n",
    "        Args:\n",
    "        input_size (int): hidden h_{i,t} column size\n",
    "        output_size (int): embedding size\n",
    "        '''\n",
    "        super(Gaussian, self).__init__()\n",
    "        self.mu_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # initialize weights\n",
    "        # nn.init.xavier_uniform_(self.mu_layer.weight)\n",
    "        # nn.init.xavier_uniform_(self.sigma_layer.weight)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        _, hidden_size = h.size()\n",
    "        sigma_t = torch.log(1 + torch.exp(self.sigma_layer(h))) + 1e-6\n",
    "        sigma_t = sigma_t.squeeze(0)\n",
    "        mu_t = self.mu_layer(h).squeeze(0)\n",
    "        return mu_t, sigma_t\n",
    "\n",
    "class NegativeBinomial(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Negative Binomial Supports Positive Count Data\n",
    "        Args:\n",
    "        input_size (int): hidden h_{i,t} column size\n",
    "        output_size (int): embedding size\n",
    "        '''\n",
    "        super(NegativeBinomial, self).__init__()\n",
    "        self.mu_layer = nn.Linear(input_size, output_size)\n",
    "        self.sigma_layer = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        _, hidden_size = h.size()\n",
    "        alpha_t = torch.log(1 + torch.exp(self.sigma_layer(h))) + 1e-6\n",
    "        mu_t = torch.log(1 + torch.exp(self.mu_layer(h)))\n",
    "        return mu_t, alpha_t\n",
    "\n",
    "def gaussian_sample(mu, sigma):\n",
    "    '''\n",
    "    Gaussian Sample\n",
    "    Args:\n",
    "    ytrue (array like)\n",
    "    mu (array like)\n",
    "    sigma (array like): standard deviation\n",
    "\n",
    "    gaussian maximum likelihood using log \n",
    "        l_{G} (z|mu, sigma) = (2 * pi * sigma^2)^(-0.5) * exp(- (z - mu)^2 / (2 * sigma^2))\n",
    "    '''\n",
    "    # likelihood = (2 * np.pi * sigma ** 2) ** (-0.5) * \\\n",
    "    #         torch.exp((- (ytrue - mu) ** 2) / (2 * sigma ** 2))\n",
    "    # return likelihood\n",
    "    gaussian = torch.distributions.normal.Normal(mu, sigma)\n",
    "    ypred = gaussian.sample(mu.size())\n",
    "    return ypred\n",
    "\n",
    "def negative_binomial_sample(mu, alpha):\n",
    "    '''\n",
    "    Negative Binomial Sample\n",
    "    Args:\n",
    "    ytrue (array like)\n",
    "    mu (array like)\n",
    "    alpha (array like)\n",
    "\n",
    "    maximuze log l_{nb} = log Gamma(z + 1/alpha) - log Gamma(z + 1) - log Gamma(1 / alpha)\n",
    "                - 1 / alpha * log (1 + alpha * mu) + z * log (alpha * mu / (1 + alpha * mu))\n",
    "\n",
    "    minimize loss = - log l_{nb}\n",
    "\n",
    "    Note: torch.lgamma: log Gamma function\n",
    "    '''\n",
    "    var = mu + mu * mu * alpha\n",
    "    ypred = mu + torch.randn(mu.size()) * torch.sqrt(var)\n",
    "    return ypred\n",
    "\n",
    "class DeepAR(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, lr=1e-3, likelihood=\"g\"):\n",
    "        super(DeepAR, self).__init__()\n",
    "\n",
    "        # network\n",
    "        self.input_embed = nn.Linear(1, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size+input_size, hidden_size, \\\n",
    "                num_layers, bias=True, batch_first=True)\n",
    "        if likelihood == \"g\":\n",
    "            self.likelihood_layer = Gaussian(hidden_size, 1)\n",
    "        elif likelihood == \"nb\":\n",
    "            self.likelihood_layer = NegativeBinomial(hidden_size, 1)\n",
    "        self.likelihood = likelihood\n",
    "    \n",
    "    def forward(self, X, y, Xf):\n",
    "        '''\n",
    "        Args:\n",
    "        X (array like): shape (num_time_series, seq_len, input_size)\n",
    "        y (array like): shape (num_time_series, seq_len)\n",
    "        Xf (array like): shape (num_time_series, horizon, input_size)\n",
    "        Return:\n",
    "        mu (array like): shape (batch_size, seq_len)\n",
    "        sigma (array like): shape (batch_size, seq_len)\n",
    "        '''\n",
    "        if isinstance(X, type(np.empty(2))):\n",
    "            X = torch.from_numpy(X).float()\n",
    "            y = torch.from_numpy(y).float()\n",
    "            Xf = torch.from_numpy(Xf).float()\n",
    "        num_ts, seq_len, _ = X.size()\n",
    "        _, output_horizon, num_features = Xf.size()\n",
    "        ynext = None\n",
    "        ypred = []\n",
    "        mus = []\n",
    "        sigmas = []\n",
    "        h, c = None, None\n",
    "        for s in range(seq_len + output_horizon):\n",
    "            if s < seq_len:\n",
    "                ynext = y[:, s].view(-1, 1)\n",
    "                yembed = self.input_embed(ynext).view(num_ts, -1)\n",
    "                x = X[:, s, :].view(num_ts, -1)\n",
    "            else:\n",
    "                yembed = self.input_embed(ynext).view(num_ts, -1)\n",
    "                x = Xf[:, s-seq_len, :].view(num_ts, -1)\n",
    "            x = torch.cat([x, yembed], dim=1) # num_ts, num_features + embedding\n",
    "            inp = x.unsqueeze(1)\n",
    "            if h is None and c is None:\n",
    "                out, (h, c) = self.encoder(inp) # h size (num_layers, num_ts, hidden_size)\n",
    "            else:\n",
    "                out, (h, c) = self.encoder(inp, (h, c))\n",
    "            hs = h[-1, :, :]\n",
    "            hs = F.relu(hs)\n",
    "            mu, sigma = self.likelihood_layer(hs)\n",
    "            mus.append(mu.view(-1, 1))\n",
    "            sigmas.append(sigma.view(-1, 1))\n",
    "            if self.likelihood == \"g\":\n",
    "                ynext = gaussian_sample(mu, sigma)\n",
    "            elif self.likelihood == \"nb\":\n",
    "                alpha_t = sigma\n",
    "                mu_t = mu\n",
    "                ynext = negative_binomial_sample(mu_t, alpha_t)\n",
    "            # if without true value, use prediction\n",
    "            if s >= seq_len - 1 and s < output_horizon + seq_len - 1:\n",
    "                ypred.append(ynext)\n",
    "        ypred = torch.cat(ypred, dim=1).view(num_ts, -1)\n",
    "        mu = torch.cat(mus, dim=1).view(num_ts, -1)\n",
    "        sigma = torch.cat(sigmas, dim=1).view(num_ts, -1)\n",
    "        return ypred, mu, sigma\n",
    "    \n",
    "def batch_generator(X, y, num_obs_to_train, seq_len, batch_size):\n",
    "    '''\n",
    "    Args:\n",
    "    X (array like): shape (num_samples, num_features, num_periods)\n",
    "    y (array like): shape (num_samples, num_periods)\n",
    "    num_obs_to_train (int):\n",
    "    seq_len (int): sequence/encoder/decoder length\n",
    "    batch_size (int)\n",
    "    '''\n",
    "    num_ts, num_periods, _ = X.shape\n",
    "    if num_ts < batch_size:\n",
    "        batch_size = num_ts\n",
    "    t = random.choice(range(num_obs_to_train, num_periods-seq_len))\n",
    "    batch = random.sample(range(num_ts), batch_size)\n",
    "    X_train_batch = X[batch, t-num_obs_to_train:t, :]\n",
    "    y_train_batch = y[batch, t-num_obs_to_train:t]\n",
    "    Xf = X[batch, t:t+seq_len]\n",
    "    yf = y[batch, t:t+seq_len]\n",
    "    return X_train_batch, y_train_batch, Xf, yf\n",
    "\n",
    "def train(\n",
    "    X, \n",
    "    y,\n",
    "    args\n",
    "    ):\n",
    "    '''\n",
    "    Args:\n",
    "    - X (array like): shape (num_samples, num_features, num_periods)\n",
    "    - y (array like): shape (num_samples, num_periods)\n",
    "    - epoches (int): number of epoches to run\n",
    "    - step_per_epoch (int): steps per epoch to run\n",
    "    - seq_len (int): output horizon\n",
    "    - likelihood (str): what type of likelihood to use, default is gaussian\n",
    "    - num_skus_to_show (int): how many skus to show in test phase\n",
    "    - num_results_to_sample (int): how many samples in test phase as prediction\n",
    "    '''\n",
    "    num_ts, num_periods, num_features = X.shape\n",
    "    model = DeepAR(num_features, args.embedding_size, \n",
    "        args.hidden_size, args.n_layers, args.lr, args.likelihood)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    random.seed(2)\n",
    "    # select sku with most top n quantities \n",
    "    Xtr, ytr, Xte, yte = util.train_test_split(X, y)\n",
    "    losses = []\n",
    "    cnt = 0\n",
    "\n",
    "    yscaler = None\n",
    "    if args.standard_scaler:\n",
    "        yscaler = util.StandardScaler()\n",
    "    elif args.log_scaler:\n",
    "        yscaler = util.LogScaler()\n",
    "    elif args.mean_scaler:\n",
    "        yscaler = util.MeanScaler()\n",
    "    if yscaler is not None:\n",
    "        ytr = yscaler.fit_transform(ytr)\n",
    "\n",
    "    # training\n",
    "    seq_len = args.seq_len\n",
    "    num_obs_to_train = args.num_obs_to_train\n",
    "    progress = ProgressBar()\n",
    "    for epoch in progress(range(args.num_epoches)):\n",
    "        # print(\"Epoch {} starts...\".format(epoch))\n",
    "        for step in range(args.step_per_epoch):\n",
    "            Xtrain, ytrain, Xf, yf = batch_generator(Xtr, ytr, num_obs_to_train, seq_len, args.batch_size)\n",
    "            Xtrain_tensor = torch.from_numpy(Xtrain).float()\n",
    "            ytrain_tensor = torch.from_numpy(ytrain).float()\n",
    "            Xf = torch.from_numpy(Xf).float()  \n",
    "            yf = torch.from_numpy(yf).float()\n",
    "            ypred, mu, sigma = model(Xtrain_tensor, ytrain_tensor, Xf)\n",
    "            # ypred_rho = ypred\n",
    "            # e = ypred_rho - yf\n",
    "            # loss = torch.max(rho * e, (rho - 1) * e).mean()\n",
    "            ## gaussian loss\n",
    "            ytrain_tensor = torch.cat([ytrain_tensor, yf], dim=1)\n",
    "            if args.likelihood == \"g\":\n",
    "                loss = util.gaussian_likelihood_loss(ytrain_tensor, mu, sigma)\n",
    "            elif args.likelihood == \"nb\":\n",
    "                loss = util.negative_binomial_loss(ytrain_tensor, mu, sigma)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cnt += 1\n",
    "    \n",
    "    # test \n",
    "    mape_list = []\n",
    "    # select skus with most top K\n",
    "    X_test = Xte[:, -seq_len-num_obs_to_train:-seq_len, :].reshape((num_ts, -1, num_features))\n",
    "    Xf_test = Xte[:, -seq_len:, :].reshape((num_ts, -1, num_features))\n",
    "    y_test = yte[:, -seq_len-num_obs_to_train:-seq_len].reshape((num_ts, -1))\n",
    "    yf_test = yte[:, -seq_len:].reshape((num_ts, -1))\n",
    "    if yscaler is not None:\n",
    "        y_test = yscaler.transform(y_test)\n",
    "    result = []\n",
    "    n_samples = args.sample_size\n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        y_pred, _, _ = model(X_test, y_test, Xf_test)\n",
    "        y_pred = y_pred.data.numpy()\n",
    "        if yscaler is not None:\n",
    "            y_pred = yscaler.inverse_transform(y_pred)\n",
    "        result.append(y_pred.reshape((-1, 1)))\n",
    "    \n",
    "    result = np.concatenate(result, axis=1)\n",
    "    p50 = np.quantile(result, 0.5, axis=1)\n",
    "    p90 = np.quantile(result, 0.9, axis=1)\n",
    "    p10 = np.quantile(result, 0.1, axis=1)\n",
    "    \n",
    "    mape = util.MAPE(yf_test, p50)\n",
    "    print(\"P50 MAPE: {}\".format(mape))\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    if args.show_plot:\n",
    "        plt.figure(1, figsize=(20, 5))\n",
    "        plt.plot([k + seq_len + num_obs_to_train - seq_len \\\n",
    "            for k in range(seq_len)], p50, \"r-\")\n",
    "        plt.fill_between(x=[k + seq_len + num_obs_to_train - seq_len for k in range(seq_len)], \\\n",
    "            y1=p10, y2=p90, alpha=0.5)\n",
    "        plt.title('Prediction uncertainty')\n",
    "        yplot = yte[-1, -seq_len-num_obs_to_train:]\n",
    "        plt.plot(range(len(yplot)), yplot, \"k-\")\n",
    "        plt.legend([\"P50 forecast\", \"true\", \"P10-P90 quantile\"], loc=\"upper left\")\n",
    "        ymin, ymax = plt.ylim()\n",
    "        plt.vlines(seq_len + num_obs_to_train - seq_len, ymin, ymax, color=\"blue\", linestyles=\"dashed\", linewidth=2)\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.xlabel(\"Periods\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.show()\n",
    "    return losses, mape_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_epoches\", \"-e\", type=int, default=1000)\n",
    "    parser.add_argument(\"--step_per_epoch\", \"-spe\", type=int, default=2)\n",
    "    parser.add_argument(\"-lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--n_layers\", \"-nl\", type=int, default=3)\n",
    "    parser.add_argument(\"--hidden_size\", \"-hs\", type=int, default=64)\n",
    "    parser.add_argument(\"--embedding_size\", \"-es\", type=int, default=64)\n",
    "    parser.add_argument(\"--likelihood\", \"-l\", type=str, default=\"g\")\n",
    "    parser.add_argument(\"--seq_len\", \"-sl\", type=int, default=7)\n",
    "    parser.add_argument(\"--num_obs_to_train\", \"-not\", type=int, default=1)\n",
    "    parser.add_argument(\"--num_results_to_sample\", \"-nrs\", type=int, default=10)\n",
    "    parser.add_argument(\"--show_plot\", \"-sp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--run_test\", \"-rt\", action=\"store_true\")\n",
    "    parser.add_argument(\"--standard_scaler\", \"-ss\", action=\"store_true\")\n",
    "    parser.add_argument(\"--log_scaler\", \"-ls\", action=\"store_true\")\n",
    "    parser.add_argument(\"--mean_scaler\", \"-ms\", action=\"store_true\")\n",
    "    parser.add_argument(\"--batch_size\", \"-b\", type=int, default=64)\n",
    "    parser.add_argument(\"--sample_size\", type=int, default=100)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.run_test:\n",
    "        data_path = util.get_data_path()\n",
    "        data = pd.read_csv(os.path.join(data_path, \"LD_MT200_hour.csv\"), parse_dates=[\"date\"])\n",
    "        data[\"year\"] = data[\"date\"].apply(lambda x: x.year)\n",
    "        data[\"day_of_week\"] = data[\"date\"].apply(lambda x: x.dayofweek)\n",
    "        data = data.loc[(data[\"date\"] >= date(2014, 1, 1)) & (data[\"date\"] <= date(2014, 3, 1))]\n",
    "\n",
    "        features = [\"hour\", \"day_of_week\"]\n",
    "        # hours = pd.get_dummies(data[\"hour\"])\n",
    "        # dows = pd.get_dummies(data[\"day_of_week\"])\n",
    "        hours = data[\"hour\"]\n",
    "        dows = data[\"day_of_week\"]\n",
    "        X = np.c_[np.asarray(hours), np.asarray(dows)]\n",
    "        num_features = X.shape[1]\n",
    "        num_periods = len(data)\n",
    "        X = np.asarray(X).reshape((-1, num_periods, num_features))\n",
    "        y = np.asarray(data[\"MT_200\"]).reshape((-1, num_periods))\n",
    "        # X = np.tile(X, (10, 1, 1))\n",
    "        # y = np.tile(y, (10, 1))\n",
    "        losses, mape_list = train(X, y, args)\n",
    "        if args.show_plot:\n",
    "            plt.plot(range(len(losses)), losses, \"k-\")\n",
    "            plt.xlabel(\"Period\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "env_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
